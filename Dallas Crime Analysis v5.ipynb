{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Objective of this notebook is to :  \n",
    "- Analyze most severe crimes in Dallas.  \n",
    "- To find the locations where such crimes happens frequently as compared to other neighborhood. \n",
    "- To see the relationship between crimes and month,year,location type. \n",
    "- Visualize the distribution of crime using maps.  \n",
    "- Create model for classification of crime data.  \n",
    "- Evaluate the accuracy of models developed.  \n",
    "- Analyze and discuss the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Table of contents</h1>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "    <ol>\n",
    "        <li><a href=\"#load_dataset\">Load the Dallas Crime data</a></li>\n",
    "        <li><a href=\"#analyze\">Analyze the Dallas Crime data</a></li>\n",
    "        <li><a href=\"#map\">Visualize Crime on Map</a></li>\n",
    "        <li><a href=\"#modeling\">Modeling</a></li>\n",
    "        <li><a href=\"#evaluation\">Evaluation</a></li>\n",
    "    </ol>\n",
    "</div>\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries used in the notebook\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import folium\n",
    "from IPython.display import Image\n",
    "from IPython.display import FileLink, FileLinks\n",
    "\n",
    "#from geopy.geocoders import Nominatim\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Matplotlib and associated plotting modules\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt \n",
    "import holoviews as hv\n",
    "import hvplot.pandas\n",
    "hv.extension ('bokeh' , 'matplotlib')\n",
    "\n",
    "#modules for modelling & accuracy \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "#!conda install -c conda-forge/label/cf201901 xgboost -y #if xgboost error is found uncomment this line \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"load_dataset\">Load the Dallas Crime data</h2>  \n",
    "This section describes data sources and its properties. Dallas crime data is available publicily at following link :\n",
    "\n",
    "https://www.dallasopendata.com/Public-Safety/Police-Incident-location-and-pertinent-information/v3r6-776m\n",
    "\n",
    "Please download CSV file and place it in the same folder from where this notebook is being run. File name should be **\"Police_Incidents.csv\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to download uncomment the below line\n",
    "#!wget https://www.dallasopendata.com/api/views/qv6i-rri7/rows.csv?accessType=DOWNLOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_read_input=pd.read_csv(\"Police_Incidents_References.csv\") #This file is being used for reference only \n",
    "\n",
    "df_read_input=pd.read_csv(\"Police_Incidents.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV file is with headers so it makes life easier. Let's look at the number of records and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of rows \n",
    "\n",
    "df_read_input.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Columns\n",
    "\n",
    "df_read_input.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick overlook of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read_input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen from above rows that the crime data is not in an ideal condition for any analysis. It contains many null values and some columns might not be required for our current scope of studeis, the data types compatibility for statistical analysis. \n",
    "\n",
    "Source data has more than fifty thousand rows and around one hundred columns. Let us explore the data types of each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 120)\n",
    "df_read_input.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data types are mostly object except for few floats like Year of Occurance, Age etc. We will change the data type of required fields once we sort them.  \n",
    "Now let us check the number of nulls in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read_input.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is locaiton based analysis we will remove the rows in which locaiton is not provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows will null values \n",
    "\n",
    "df_read_input.dropna(axis=0,  inplace=True,subset=['Location1','Type of Incident'])\n",
    "#df_read_input =df_read_input.iloc[:10000,:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check how many rows got affected due to \n",
    "\n",
    "df_read_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create a bucket for type of incidents reported, this will make it easier to identify each crime's type, corresponding to it's rank and severity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Few example type of incidents are listed below \n",
    "\n",
    "df_read_input['Type of Incident'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function which will return us the bucket in which each incident belongs. Also we will use this oppurtunity to assign rank and severity to each incident type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for incident type bucket \n",
    "def func_incident_type(data):\n",
    "    type1,  rank,severity = func_incident_values(data)\n",
    "    return type1\n",
    "\n",
    "#function for rank of crime\n",
    "def func_rank(data):\n",
    "    type1,  rank,severity = func_incident_values(data)\n",
    "    return rank\n",
    "\n",
    "#function for severity of crime \n",
    "def func_severity(data):\n",
    "    type1,  rank,severity = func_incident_values(data)\n",
    "    return severity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declare the incident buckets, its rank and its severity \n",
    "\n",
    "Incident_bucket1 = 'ACCIDENT / RANDOM / ? / OTHER / NO OFFENSE' # Incidnet bucket\n",
    "r1=4 #rank of crime \n",
    "s1=19 #severity of crime \n",
    "Incident_bucket2 = 'THEFT RELATED'\n",
    "r2=1\n",
    "s2=4\n",
    "Incident_bucket3 = 'FRAUD / ORGANIZED CRIME / FALSE INFO'\n",
    "r3=3\n",
    "s3=14\n",
    "Incident_bucket4 = 'TRAFFIC RELATED'\n",
    "r4=2\n",
    "s4=9\n",
    "Incident_bucket5 = 'CRIMINAL MISHIEF / DAMAGE OF PROPERTY ARSON'\n",
    "r5=3\n",
    "s5=12\n",
    "Incident_bucket6 = 'HARASSMENT / KIDNAPPING HOSTAGE'\n",
    "r6=3\n",
    "s6=11\n",
    "Incident_bucket7 = 'ASSAULT RELATED'\n",
    "r7=2\n",
    "s7=3\n",
    "Incident_bucket8 = 'EVADING ARREST RELATED'\n",
    "r8=4\n",
    "s8=17\n",
    "Incident_bucket9 = 'THREAT / SECURITY BREACH / ALARM INCIDENT'\n",
    "r9=2\n",
    "s9=8\n",
    "Incident_bucket10 = 'DEADLY CONDUCT'\n",
    "r10=1\n",
    "s10=2\n",
    "Incident_bucket11 = 'MURDER RELATED'\n",
    "r11=1\n",
    "s11=1\n",
    "Incident_bucket12 = 'GUN / WEAPON RELATED'\n",
    "r12=1\n",
    "s12=5\n",
    "Incident_bucket13 = 'ANIMAL / LITTERING RELATED'\n",
    "r13=4\n",
    "s13=16\n",
    "Incident_bucket14 = 'DRUG RELATED'\n",
    "r14=2\n",
    "s14=6\n",
    "Incident_bucket15 = 'MANSLAUGHTER'\n",
    "r15=2\n",
    "s15=7\n",
    "Incident_bucket16 = 'INTOXICATION / ALCOHOL RELATED'\n",
    "r16=3\n",
    "s16=13\n",
    "Incident_bucket17 = 'CUSTODY / COURT / BOND / INTERFERENCE / WARRANT RELATED'\n",
    "r17=4\n",
    "s17=18\n",
    "\n",
    "\n",
    "#Main function which processes data to return Incident bucket, Rank and Severity in Order\n",
    "\n",
    "def func_incident_values(data):   \n",
    "    \n",
    "   #As per Refernce provided create a list of possible crimes and assign bucket to them \n",
    "    if ('THEFT' in data) or ('BMV' in data) or ('BURGLARY' in data) or ('ROBBERY' in data):\n",
    "        return Incident_bucket2,r2,s2\n",
    "    elif ('FRAUD' in data) or ('ORGANIZED CRIME' in data)or ('FALSE INFO' in data) :\n",
    "        return Incident_bucket3,r3,s3 \n",
    "    elif ('TRAFFIC' in data):\n",
    "        return Incident_bucket4,r4,s4\n",
    "    elif ('CRIMINAL MISHIEF' in data) or ('DAMAGE OF PROPERTY ARSON' in data) or ('CRIM MISCHIEF' in data ):\n",
    "        return Incident_bucket5,r5,s5\n",
    "    elif ('HARASSMENT ' in data) or ('KIDNAPPING' in data)or ('HOSTAGE' in data) or ('STALKING' in data):\n",
    "        return Incident_bucket6,r6,s6\n",
    "    elif ('ASSAULT' in data) :\n",
    "        return Incident_bucket7,r7,s7\n",
    "    elif ('EVADING ARREST' in data) :\n",
    "        return Incident_bucket8,r8,s8\n",
    "    elif ('THREAT' in data) or ('SECURITY BREACH' in data)or ('ALARM INCIDENT' in data):\n",
    "        return Incident_bucket9,r9,s9\n",
    "    elif ('DEADLY CONDUCT' in data) :\n",
    "        return Incident_bucket10,r10,s10\n",
    "    elif ('MURDER' in data) :\n",
    "        return Incident_bucket11,r11,s11   \n",
    "    elif ('GUN' in data) or ('WEAPON' in data):\n",
    "        return Incident_bucket12,r12,s12    \n",
    "    elif ('ANIMAL' in data) or ('LITTERING' in data):\n",
    "        return Incident_bucket13,r13,s13 \n",
    "    elif ('DRUG' in data) :\n",
    "        return Incident_bucket14,r14,s14\n",
    "    elif ('MANSLAUGHTER' in data):\n",
    "        return Incident_bucket15,r15,s15\n",
    "    elif ('INTOXICATION' in data) or ('ALCOHOL' in data):\n",
    "        return Incident_bucket16,r16,s16\n",
    "    elif ('CUSTODY' in data) or ('COURT' in data)or ('BOND' in data) or ('INTERFERENCE' in data) or ('WARRANT' in data):\n",
    "        return Incident_bucket17,r17,s17\n",
    "    else :\n",
    "        #rest of the crimes are listed in this category \n",
    "        return Incident_bucket1,r1,s1\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since functions are created now let us call each function for each row to get our desried result in dataframe columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new column for Type_of_Incident_Bucket\n",
    "df_read_input['Type_of_Incident_Bucket'] = df_read_input['Type of Incident'].apply(lambda x:func_incident_type(x))\n",
    "\n",
    "#Use our created function to rank each crime row \n",
    "df_read_input['rank']= df_read_input['Type of Incident'].apply(lambda x:func_rank(x))\n",
    "\n",
    "#Assign severity to each crime using function func_severity\n",
    "df_read_input['severity']=df_read_input['Type of Incident'].apply(lambda x:func_severity(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us analyze average severity, amount of crimes using data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read_input['severity'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data shows us the mean value of severity of crime in dallas. Please note that median for crime severity is very different then its mean value.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this study focus is on severe crimes so let us fliter top most severe crimes rows. This will also filter out rows as the number of rows imported is large. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_crimes_count = 3\n",
    "\n",
    "#Filter rows based on crime severity \n",
    "df_read_input= df_read_input[df_read_input['severity']<=to_crimes_count ]\n",
    "df_read_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from above shape that the number of rows have reduced much.  \n",
    "\n",
    "Location type bucket creation : This will allow us to provide more better insight of what kind of locations are being targeted in such crimes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get location types as per reference \n",
    "\n",
    "def func_loc_type(data):\n",
    "    if ( 'Government Facility' in data): return 'Government Facility'\n",
    "    elif ( 'Residential Area' in data): return 'Residential Area'\n",
    "    elif ( 'Parking Lot' in data)or (( 'Parking' in data)): return 'Parking Lot'\n",
    "    elif ( 'Street' in data): return 'Street'\n",
    "    elif ( 'Business Office' in data): return 'Business Office'\n",
    "    elif ( 'Retail Store' in data) or ( 'Retail' in data) or ( 'Store' in data): return 'Retail Store'\n",
    "    elif ( 'Apartment Complex' in data) or ( 'Apartment' in data) or ( 'Complex' in data): return 'Apartment Complex'\n",
    "    elif ( 'Hotel' in data): return 'Hotel'\n",
    "    elif ( 'Commercial Property' in data) or ( 'Commercial' in data): return 'Commercial Property'\n",
    "    elif ( 'ATM / Bank' in data) or ( 'ATM' in data) or ( 'Bank' in data): return 'ATM / Bank'\n",
    "    elif ( 'Pharmacy' in data): return 'Pharmacy'\n",
    "    elif ( 'Entertainment/Sports Venue' in data)or ( 'Entertainment' in data) or ( 'Sports' in data): return 'Entertainment/Sports Venue'\n",
    "    elif ( 'Park' in data): return 'Park'\n",
    "    elif ( 'Cyberspace' in data): return 'Cyberspace'\n",
    "    elif ( 'Financial Institution' in data)or ( 'Financial' in data): return 'Financial Institution'\n",
    "    elif ( 'Restraunt' in data): return 'Restraunt'\n",
    "    elif ( 'Construction' in data) or ( 'Manufacturing' in data): return 'Construction / Manufacturing Site'\n",
    "    elif ( 'Bar' in data): return 'Bar'\n",
    "    elif ( 'School' in data): return 'School'\n",
    "    elif ( 'Agricultural Area' in data)or ( 'Agricultural' in data): return 'Agricultural Area'\n",
    "    elif ( 'Corrections Facility' in data): return 'Corrections Facility'\n",
    "    elif ( 'Storage Facility' in data): return 'Storage Facility'\n",
    "    elif ( 'Hospital' in data): return 'Hospital'\n",
    "    elif ( 'Airport' in data): return 'Airport'\n",
    "    elif ( 'Religious Building' in data) or ( 'Religious' in data): return 'Religious Building'\n",
    "    elif ( 'Gas Station' in data) or ( 'Gas' in data): return 'Gas Station'\n",
    "    elif ( 'City Property' in data): return 'City Property'\n",
    "    else  : return 'Other'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the function to create types of location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Type_of_Location \n",
    "df_read_input['Type_of_Location']=df_read_input['Type  Location'].apply(lambda x:func_loc_type(str(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us extract the coordinates from location field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Location contains coordinates in brackets so we will split by bracket and then split further based on comma to get latitude and longitude\n",
    "\n",
    "df_read_input[['Location2','coordinates2']]=df_read_input['Location1'].str.split('(',expand=True)\n",
    "df_read_input[['latitude','longitude']]=df_read_input['coordinates2'].str.split(',',expand=True)\n",
    "df_read_input[['longitude','temp']]=df_read_input['longitude'].str.split(')',expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a look at if the values are successfully extracted \n",
    "\n",
    "df_read_input[['Location1','latitude','longitude']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Hour from Time of Occurance for analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read_input[['Hour','Minute']]=df_read_input['Time1 of Occurrence'].str.split(':',expand=True)\n",
    "\n",
    "#Check if extraction worked\n",
    "\n",
    "df_read_input[['Time1 of Occurrence','Hour']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"analyze\">Analyze the Dallas Crime data</h2> \n",
    "In this section we will analyze different factors which impact crime severity consisting of following columns :\n",
    "\n",
    "'latitude','longitude','Type_of_Incident_Bucket','rank','severity','Zip Code','Type_of_Location','Month1 of Occurence','Hour','Division','Council District'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incidents=df_read_input[['latitude','longitude','Type_of_Incident_Bucket','rank','severity','Zip Code','Type_of_Location','Month1 of Occurence','Hour','Division','Council District','Offense Status']].copy()\n",
    "\n",
    "#Drop null rows and create a new column count, this column will help us in counting rows in dataframe\n",
    "\n",
    "df_incidents.dropna(inplace=True)\n",
    "df_incidents['count']=1\n",
    "df_incidents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we will analyze the data types of above columns. This is requried since it will affect our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incidents.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will change these columns data types into float and int as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incidents.latitude=df_incidents.latitude.astype(float)\n",
    "df_incidents.longitude=df_incidents.longitude.astype(float)\n",
    "df_incidents.rank=df_incidents['rank'].astype(float)\n",
    "df_incidents.severity=df_incidents.severity.astype(int)\n",
    "df_incidents['Zip Code']=df_incidents['Zip Code'].astype(int)\n",
    "df_incidents['Hour']=df_incidents['Hour'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we will look at the number of crimes in each severity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_incidents[['severity','count']].groupby('severity').count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that most sever crime count is much less than second and third severe crime. Next we will analyze the hourly data to see any trend. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path=\"webapp/static/images/\"\n",
    "\n",
    "df_hourly=df_incidents.groupby('Hour').count()\n",
    "\n",
    "df_hourly['count'].plot(kind='bar', figsize=(12, 6))\n",
    "plt.ylabel(\"Count of Crimes\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.title(\"Time Vs Crime \")\n",
    "\n",
    "plt.legend()\n",
    "#plt.show()\n",
    "plt.savefig(image_path+'df_hourly.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph shows that when the time is 18 Hours then maximum number of crimes take place. Hours between 1-7 have the least number of criminal activities going on. Next let us visualize the location types where these crimes occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Type_of_Location=df_incidents.groupby('Type_of_Location').count()\n",
    "\n",
    "df_Type_of_Location=df_Type_of_Location.sort_values('count')\n",
    "df_Type_of_Location['count'].plot(kind='barh', figsize=(12, 6))\n",
    "plt.xlabel(\"Count of Crimes\")\n",
    "plt.ylabel(\"Locations\")\n",
    "plt.title(\"Locations Vs Crime \")\n",
    "\n",
    "plt.legend()\n",
    "#plt.show()\n",
    "plt.savefig(image_path+'df_Type_of_Location.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph reveals that street, apartment complex and parking lot are the most dangerous place to be in case of crime prediction. Next we will focus on Dallas division where major crimes occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_division=df_incidents.groupby('Division').count()\n",
    "\n",
    "df_division=df_division.sort_values('count')\n",
    "df_division['count'].plot(kind='barh', figsize=(10, 5))\n",
    "plt.xlabel(\"Count of Crimes\")\n",
    "plt.ylabel(\"Divisions\")\n",
    "plt.title(\"Divisions Vs Crime \")\n",
    "\n",
    "plt.legend()\n",
    "#plt.show()\n",
    "plt.savefig(image_path+'df_division.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen by the graph Southeast division is victim of most crimes while north central is compartively safer in terms of severe crimes. Now we will analyze Council wise crime distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_council=df_incidents.groupby('Council District').count()\n",
    "\n",
    "df_council=df_council.sort_values('count')\n",
    "df_council['count'].plot(kind='bar', figsize=(12, 6))\n",
    "plt.ylabel(\"Count of Crimes\")\n",
    "plt.xlabel(\"Council District\")\n",
    "plt.title(\"Council District Vs Crime \")\n",
    "\n",
    "plt.legend()\n",
    "#plt.show()\n",
    "plt.savefig(image_path+'df_council.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per above graph Council D7 takes the lead for severe crime. Next we will analyze the month in which most crimes happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_month=df_incidents.groupby('Month1 of Occurence').count()\n",
    "df_month=df_month.sort_values('count')\n",
    "df_month['count'].plot(kind='bar', figsize=(12, 6))\n",
    "plt.ylabel(\"Count of Crimes\")\n",
    "plt.xlabel(\"Month of Occurence\")\n",
    "plt.title(\"Month of Occurence Vs Crime \")\n",
    "\n",
    "plt.legend()\n",
    "#plt.show()\n",
    "plt.savefig(image_path+'df_month.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bars are almost equal in height and it is not clear from the graph so let us see the values of montly crime in dallas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_month['count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that maximum number of crimes is being committed in the month of July while February has the least number. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the 'Offense Status' below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offense=df_incidents.groupby(['Month1 of Occurence', 'Offense Status'])['count'].count()\n",
    "\n",
    "by_month = df_offense.hvplot.bar('Month1 of Occurence', groupby='Offense Status', width=700, dynamic=False)\n",
    "#by_month.save('df_offense.png')  \n",
    "hv.save(by_month,'df_offense.png')\n",
    "by_month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the crimes based on the offense status and looking at their trend during the month of the year. The number of crimes which are clear by arrest is lowest in the month of January and Febraury. The highest number of crimes cleared by exceptional arrest are noted to be in the month of July. \n",
    "June, August and December have the lowest crimes cases closed.While, May has the highest open crime cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def nansum(a):\n",
    "    return np.nan if np.isnan(a).all() else np.nansum(a)\n",
    "\n",
    "heatmap= df_incidents.hvplot.heatmap('Council District', 'Offense Status', 'count',reduce_function=nansum,\n",
    "                       flip_yaxis=True, xaxis=True, logz=True,  height= 300, width =900)\n",
    "hv.save(heatmap,'df_council_heatmap.png')\n",
    "heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This heatmap shows the Council District wise offense status. District D7 has the highest number of suspended cases, following by D4, D8, D2 and D6. There is no signigicant variation in other offense status across the districts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"map\">Visualize Crime on Map</h2> \n",
    "\n",
    "In this section we will use folium maps to locate crime in Dallas. This gives a better understanding of which locations are more prone to severe crimes.  We will start by plotting a simple map of Dallas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dallas latitude and longitude values\n",
    "latitude = 32.77\n",
    "longitude = -96.70\n",
    "\n",
    "\n",
    "html_path = \"webapp/templates/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the crime data is very large, Plotting all at once will take large resources. Therefore we will plot maps for each crime severity, starting with severity level one crime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_severity_one =df_incidents[df_incidents.severity ==1]\n",
    "df_severity_one.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from folium import plugins\n",
    "\n",
    "# let's start again with a clean copy of the map of San Dallas\n",
    "dallas_map = folium.Map(location = [latitude, longitude], zoom_start = 11)\n",
    "\n",
    "# instantiate a mark cluster object for the incidents in the dataframe\n",
    "incidents = plugins.MarkerCluster().add_to(dallas_map)\n",
    "\n",
    "# loop through the dataframe and add each data point to the mark cluster\n",
    "for lat, lng, label, in zip(df_severity_one.latitude, df_severity_one.longitude, df_severity_one['Type_of_Incident_Bucket']):\n",
    "    folium.Marker(\n",
    "        location=[lat, lng],\n",
    "        icon=None,\n",
    "        popup=str(label),\n",
    "    ).add_to(incidents)\n",
    "    \n",
    "\n",
    "# display map\n",
    "dallas_map.save(outfile= html_path+'dallas_severity1_crime_map.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above map shows severity level one crime locations in Dallas. Up next we will show severitty level two crime on map. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Image(\"severity1.jpg\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on the link below to see the interactive map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FileLink(html_path+'dallas_severity1_crime_map.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_severity_two =df_incidents[df_incidents.severity ==2]\n",
    "df_severity_two.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium import plugins\n",
    "\n",
    "# let's start again with a clean copy of the map of San Dallas\n",
    "dallas_map = folium.Map(location = [latitude, longitude], zoom_start = 10)\n",
    "\n",
    "# instantiate a mark cluster object for the incidents in the dataframe\n",
    "incidents = plugins.MarkerCluster().add_to(dallas_map)\n",
    "\n",
    "# loop through the dataframe and add each data point to the mark cluster\n",
    "for lat, lng, label, in zip(df_severity_two.latitude, df_severity_two.longitude, df_severity_two['Type_of_Incident_Bucket']):\n",
    "    folium.Marker(\n",
    "        location=[lat, lng],\n",
    "        icon=None,\n",
    "        popup=str(label),\n",
    "    ).add_to(incidents)\n",
    "    \n",
    "\n",
    "# display map\n",
    "dallas_map.save(outfile=html_path+'dallas_severity2_crime_map.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Image(\"severity2.jpg\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on the link below to see the interactive map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FileLink(html_path+'dallas_severity2_crime_map.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_severity_three =df_incidents[df_incidents.severity ==3]\n",
    "df_severity_three.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from folium import plugins\n",
    "\n",
    "# let's start again with a clean copy of the map of San Dallas\n",
    "dallas_map = folium.Map(location = [latitude, longitude], zoom_start = 11)\n",
    "\n",
    "# instantiate a mark cluster object for the incidents in the dataframe\n",
    "incidents = plugins.MarkerCluster().add_to(dallas_map)\n",
    "\n",
    "# loop through the dataframe and add each data point to the mark cluster\n",
    "for lat, lng, label, in zip(df_severity_three.latitude, df_severity_three.longitude, df_severity_three['Type_of_Incident_Bucket']):\n",
    "    folium.Marker(\n",
    "        location=[lat, lng],\n",
    "        icon=None,\n",
    "        popup=str(label),\n",
    "    ).add_to(incidents)\n",
    "    \n",
    "\n",
    "# display map\n",
    "dallas_map.save(outfile=html_path+'dallas_severity3_crime_map.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Image(\"severity3.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on the link below to see the interactive map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileLink(html_path+'dallas_severity3_crime_map.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see that folium is not performing well once the count of crime goes beyond three thousand so we will find another method to visualize this data but first data will be prepared Council District wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Group by Distrcit Council \n",
    "\n",
    "df_zip_grouped= df_incidents[['Council District','count']]\n",
    "df_zip_grouped=df_zip_grouped.groupby('Council District').sum()\n",
    "\n",
    "df_zip_grouped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choropleth Maps <a id=\"choropleth\"></a>\n",
    "\n",
    "A `Choropleth` map is a thematic map in which areas are shaded or patterned in proportion to the measurement of the statistical variable being displayed on the map, such as population density or per-capita income. The choropleth map provides an easy way to visualize how a measurement varies across a geographic area or it shows the level of variability within a region. Dallas crime is plotted below on choropleth map. \n",
    "And now to create a `Choropleth` map, we will use the *choropleth* method with the following main parameters:\n",
    "\n",
    "1. geo_data, which is the GeoJSON file.\n",
    "2. data, which is the dataframe containing the data.\n",
    "3. columns, which represents the columns in the dataframe that will be used to create the `Choropleth` map.\n",
    "4. key_on, which is the key or variable in the GeoJSON file that contains the name of the variable of interest. To determine that, you will need to open the GeoJSON file using any text editor and note the name of the key or variable that contains the name of the countries, since the countries are our variable of interest. In this case, **dist_name** is the key in the GeoJSON file that contains the name of the countries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Choropleth Dallas Map\n",
    "\n",
    "#Use below open link to download Dallas GeoJSON file \n",
    "#https://www.dallasopendata.com/Geography-Boundaries/Adopted-Council-Districts/6dcw-hhpj\n",
    "dallas_geo =r'Dallas.geojson' # geojson file\n",
    "\n",
    "# create a numpy array of length 6 and has linear spacing from the minium total crime to the maximum total crime\n",
    "threshold_scale = np.linspace(df_zip_grouped['count'].min(),\n",
    "                              df_zip_grouped['count'].max(),\n",
    "                              6, dtype=int)\n",
    "threshold_scale = threshold_scale.tolist() # change the numpy array to a list\n",
    "threshold_scale[-1] = threshold_scale[-1] + 1 # make sure that the last value of the list is greater than the maximum crime\n",
    "\n",
    "\n",
    "# create a plain world map\n",
    "dallas_map = folium.Map(location=[latitude, longitude], zoom_start=10, tiles='Mapbox Bright')\n",
    "dallas_map.choropleth(\n",
    "    geo_data=dallas_geo,\n",
    "    data=df_zip_grouped,\n",
    "    columns=[df_zip_grouped.index, 'count'],\n",
    "    key_on='feature.properties.dist_name',\n",
    "    threshold_scale=threshold_scale,\n",
    "    fill_color='YlOrRd', \n",
    "    fill_opacity=0.7, \n",
    "    line_opacity=0.2,\n",
    "    legend_name='Crime Count',\n",
    "    reset=True\n",
    ")\n",
    "dallas_map.save(outfile=html_path+'dallas_crime_choropleth.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a better visualization of Severe crimes of Dallas location wise. The dark red places have high crime rate and lighter color districts have low crime rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Image(\"chloropleth_dallas.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on the link below to see the interactive map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileLink(html_path+'dallas_crime_choropleth.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"modeling1\">Modeling XGBoost</h2>\n",
    "\n",
    "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.\n",
    "\n",
    "To start with modeling the feature selection will be performed and then normalization to eliminate the difference of scale or size.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_incidents.head()\n",
    "\n",
    "\n",
    "X=df_incidents[['rank','Zip Code','Type_of_Location','Month1 of Occurence','Hour']]\n",
    "X=pd.get_dummies(data=X, columns=['Type_of_Location','Month1 of Occurence'])\n",
    "X=np.asarray(X)\n",
    "X=StandardScaler().fit(X).transform(X)\n",
    "X[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Location and Months are string data type in our data so we will create convert them into numerical values using pandas feature of get_dummies. The aim here is to classify each record into correct severity level, therefore output is set to severity vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.asarray(df_incidents['severity'])\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is split into training and testing set using skitlearn function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "seed = 7\n",
    "test_size = 0.33\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First XGBoost model  dataset\n",
    "\n",
    "# fit model no training data\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"modeling2\">Modeling (SVM with Scikit-learn)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM algorithm offers a choice of kernel functions for performing its processing. Basically, mapping data into a higher dimensional space is called kernelling. The mathematical function used for the transformation is known as the kernel function, and can be of different types, such as:\n",
    "\n",
    "    1.Linear\n",
    "    2.Polynomial\n",
    "    3.Radial basis function (RBF)\n",
    "    4.Sigmoid\n",
    "Each of these functions has its characteristics, its pros and cons, and its equation, but as there's no easy way of knowing which function performs best with any given dataset, we usually choose different functions in turn and compare the results. Let's just use the default, RBF (Radial Basis Function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel='rbf')\n",
    "clf.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After being fitted, the model can then be used to predict new values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = clf.predict(X_test)\n",
    "yhat [0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"evaluation\">Evaluation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now we cleaned the data and then we created models for the data which can train on it. This is essential part of data modeling to have enough data on which training is performed. Now we will evaluate if the model is working as expected on unseen data, referred to as testing data in data science world.  \n",
    "First we will **evaluate XGBoost model's** out put here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy Score: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy score provides the number of correctly classified samples, which in case of XGBoost is high score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In pattern recognition and information retrieval with binary classification, precision (also called positive predictive value) is the fraction of retrieved instances that are relevant, while recall (also known as sensitivity) is the fraction of relevant instances that are retrieved. Both precision and recall are therefore based on an understanding and measure of relevance.\n",
    "  \n",
    "The f1-score gives you the harmonic mean of precision and recall. The scores corresponding to every class will tell you the accuracy of the classifier in classifying the data points in that particular class compared to all other classes. \n",
    "The support is the number of samples of the true response that lie in that class.\n",
    "\n",
    "Crime severity 1 is very low in number and model is not able to precisely predict its class correctly, while it gets much higher for crime with severity 2. The best part is crime with severity 3 are all correctly predicted by XGBoost model. \n",
    "\n",
    " Above figure looks promising so let us visualize it too with help of confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred, labels=[1,2,3])\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['Severity 1','Severity 2','Severity 3'],normalize= False,  title='Confusion matrix XGBoost Model')\n",
    "plt.savefig('Confusion matrix XGBoost Model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix is visual representation of how good XGBoot model is doing in predicting which crime belong to which severity level class. Severity level 3 performing the best as compared to severity level 2 or level 1.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will **evaluate SVM model's output**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print(\"Accuracy Score: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For SVM model the accuracy is almost the same as it was for XGBoot model. Let us explore which severity level perform better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (classification_report(y_test, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification report for SVM model shows us that for severity level 3 crimes precision and recall is 1, indicating model has predicted with excellent accuracy. For crime severity 2 the prediction is lower than severity 3 while severity 1 crime is not predicted correctly by SVM model either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, yhat, labels=[1,2,3])\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['Severity 1','Severity 2','Severity 3'],normalize= False,  title='Confusion matrix SVM Model')\n",
    "plt.savefig('Confusion matrix SVM Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to visualize that SVM model is very good at predicting severity 3 crimes. This comes from the fact that in our chosen data set number of crimes mostly are of severity level 3 so the model is able to train for its prediction. For severity level 2 most of the classification is correct but for severity level 1 crime SVM model is not able to classify correctly. Since they are low in numbers therefore the overall score of model accuracy is still high.  \n",
    "The diagonal line represent true positive by SVM model and dark blue color shows number of records in each class. This also gives us visual of how many severity 1 crimes are label as severity 2 (131) and severity 3(4) crimes. Similarly only 7 crimes of severity 2 crimes are label as severity 3 while for severity 3 all are predicted correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
